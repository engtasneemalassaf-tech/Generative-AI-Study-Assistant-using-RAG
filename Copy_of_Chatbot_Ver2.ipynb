{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/engtasneemalassaf-tech/Generative-AI-Study-Assistant-using-RAG/blob/main/Copy_of_Chatbot_Ver2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osBAXfbYP2YC"
      },
      "source": [
        "# Generative AI Study Assistant using RAG\n",
        "\n",
        "**Team Name:** Rahaf Kanaan, Shifaa Al-zu'bi, Thabet Zamari, Rafah Ali, Tasneem Alassaf.\n",
        "\n",
        "This Jupyter Notebook presents a Retrieval-Augmented Generation (RAG) based study assistant developed as part of a Generative AI course project.  \n",
        "The system is designed to answer student questions by retrieving relevant information from course lecture materials and generating accurate context-aware responses using large language models.\n",
        "\n",
        "The notebook demonstrates the complete pipeline, including document preprocessing, semantic retrieval, prompt engineering, and response generation.  \n",
        "Both API-based and local open-source language models are supported, allowing flexibility in experimentation while maintaining the same RAG architecture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QruO7EkAplVp"
      },
      "source": [
        "## 1. Environment Setup and Library Installation\n",
        "\n",
        "In this step, we install all the required Python libraries needed to build the Generative AI project.  \n",
        "These libraries support:\n",
        "\n",
        "- LangChain framework for building the RAG pipeline\n",
        "- Document loading and text splitting\n",
        "- Embedding models and vector storage using FAISS\n",
        "- Integration with OpenAI / GitHub Models APIs\n",
        "- Evaluation utilities and PDF processing\n",
        "\n",
        "This setup ensures that the environment contains all dependencies before starting the implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4pYBqu5j_VE"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U langchain langchain-community langchain-text-splitters\n",
        "\n",
        "!pip -q install -U faiss-cpu sentence-transformers\n",
        "\n",
        "!pip -q install -U langchain-openai tiktoken python-dotenv\n",
        "\n",
        "!pip -q install -U rouge-score\n",
        "\n",
        "!pip install pypdf\n",
        "\n",
        "!pip -q install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTMkc8iuKOkR"
      },
      "source": [
        "## 2. Language Model Selection and Initialization\n",
        "\n",
        "This step initializes the Large Language Model (LLM) used for answer generation.  \n",
        "The implementation is designed to be flexible, allowing the system to switch between:\n",
        "\n",
        "- **API-based models** (GPT-4o via GitHub Models API) for advanced reasoning and explanation.\n",
        "- **Local open-source models** (flan-t5-base) for offline inference without external API dependencies.\n",
        "\n",
        "The selection is controlled using a configuration variable (`LLM_MODE`), enabling seamless comparison between different LLM backends while keeping the rest of the RAG pipeline unchanged.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoANL3r1JFrU"
      },
      "outputs": [],
      "source": [
        "LLM_MODE = \"local\"\n",
        "\n",
        "\n",
        "if LLM_MODE == \"api\":\n",
        "    from getpass import getpass\n",
        "    import os\n",
        "    from langchain_openai import ChatOpenAI\n",
        "\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your GitHub PAT: \")\n",
        "\n",
        "    llm = ChatOpenAI(\n",
        "        model=\"gpt-4o\",\n",
        "        base_url=\"https://models.github.ai/inference/v1\",\n",
        "        api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "        temperature=0.2\n",
        "    )\n",
        "\n",
        "    print(\"LLM loaded: GPT-4o via GitHub Models API\")\n",
        "\n",
        "elif LLM_MODE == \"local\":\n",
        "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "    from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "    model_name = \"google/flan-t5-base\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "    print(\"LLM loaded: Flan-T5-base (local HuggingFace)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE-XQxbMqEoe"
      },
      "source": [
        "## 3. Uploading Course Materials (PDF Files)\n",
        "\n",
        "In this step, the course lecture slides and reference materials are uploaded to the Google Colab environment.  \n",
        "These PDF files serve as the **knowledge source** for the Retrieval-Augmented Generation (RAG) system and will later be processed, indexed, and queried by the chatbot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih4C0wTqVc2g"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbnCeJH5qOLU"
      },
      "source": [
        "## 4. Organizing Uploaded Files into a Data Directory\n",
        "\n",
        "After uploading the PDF files, they are organized into a dedicated directory (`data/`).  \n",
        "This step ensures a clean and structured project layout, making it easier to load, process, and manage the documents consistently throughout the pipeline.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPOqxz_zXyNQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "for file in os.listdir():\n",
        "    if file.endswith(\".pdf\"):\n",
        "        shutil.move(file, \"data/\" + file)\n",
        "\n",
        "print(\"Files inside data:\", os.listdir(\"data\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRWvUGXWqZiG"
      },
      "source": [
        "## 5. Loading and Parsing PDF Documents\n",
        "\n",
        "This step loads the uploaded PDF files from the data directory and extracts their textual content.  \n",
        "Each PDF is processed page by page, converting the raw documents into structured text objects that can be further analyzed and indexed by the RAG pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfvnUySxYNky"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "DATA_PATH = \"data/\"\n",
        "documents = []\n",
        "\n",
        "for file_name in os.listdir(DATA_PATH):\n",
        "    if file_name.endswith(\".pdf\"):\n",
        "        file_path = os.path.join(DATA_PATH, file_name)\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        documents.extend(loader.load())\n",
        "\n",
        "print(f\"Loaded {len(documents)} pages from PDFs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOAMkb8Vqmqf"
      },
      "source": [
        "## 6. Text Chunking for Efficient Retrieval\n",
        "\n",
        "In this step, the extracted document text is divided into smaller overlapping chunks.  \n",
        "Chunking improves retrieval accuracy by allowing the system to match user queries with the most relevant portions of the documents rather than entire pages.\n",
        "\n",
        "The overlap between chunks helps preserve contextual continuity across adjacent text segments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPR96-39ZH3O"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Created {len(chunks)} text chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjnh7fgRqwlv"
      },
      "source": [
        "## 7. Creating Embeddings and Building the Vector Store\n",
        "\n",
        "In this step, semantic embeddings are generated for each text chunk using a pre-trained sentence transformer model.  \n",
        "These embeddings represent the meaning of the text in a numerical vector space, enabling effective similarity-based retrieval.\n",
        "\n",
        "The vectors are then stored in a FAISS index, which allows fast and efficient retrieval of the most relevant document chunks in response to user queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpXfG6ABalZ0"
      },
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "vectorstore = FAISS.from_documents(\n",
        "    chunks,\n",
        "    embedding=embeddings\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg08ILnTq7sq"
      },
      "source": [
        "## 8. Prompt Engineering and Instruction Design\n",
        "\n",
        "This step defines the prompt template that controls how the language model interprets and answers user questions.  \n",
        "The prompt is carefully designed to encourage clear reasoning and explanatory responses while strictly limiting the model to the provided course context.\n",
        "\n",
        "By specifying detailed instructions, the system ensures that answers remain accurate, grounded in the retrieved documents, and free from external or hallucinated information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQtgwoCKaK_c"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a study assistant for a Generative AI course. \"\n",
        "        \"Your task is to answer questions strictly using the provided course context.\\n\\n\"\n",
        "        \"Instructions:\\n\"\n",
        "        \"- Carefully analyze the question, even if it is indirect, rephrased, or explanatory.\\n\"\n",
        "        \"- You are allowed to reason, explain, and infer logically, but ONLY using information from the context.\\n\"\n",
        "        \"- If the answer requires combining multiple parts of the context, do so clearly.\\n\"\n",
        "        \"- If the question cannot be fully answered using the context, say exactly:\\n\"\n",
        "        \"  'I don't know based on the course material.'\\n\"\n",
        "        \"- Do NOT use any external knowledge.\\n\"\n",
        "        \"- Do NOT leave the question unanswered.\\n\"\n",
        "        \"-if the question is hello, you can answer it: hello, how i can help you in the material.\\n\"\n",
        "        \"-if the question is thank you, you can answer You are welcome, have a nice day ^-^.\\n\"\n",
        "        \"- if the question is goodbye, you can answer it: goodbye, have a nice day.\\n\"\n",
        "        \"-if the question is exit, you can answer it: goodbye, have a nice day.\"\n",
        "    ),\n",
        "\n",
        "    (\n",
        "\n",
        "        \"human\",\n",
        "        \"Context:\\n{context}\\n\\nQuestion:\\n{question}\\n\\n\"\n",
        "        \"Please provide a clear, step-by-step explanation.\"\n",
        "    )\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xUHIEDMrL_3"
      },
      "source": [
        "## 9. Constructing the Retrieval-Augmented Generation (RAG) Pipeline\n",
        "\n",
        "This step combines all previously defined components into a single Retrieval-Augmented Generation (RAG) pipeline.  \n",
        "The pipeline retrieves the most relevant document chunks based on the user query, injects them into the prompt as context, and then generates a grounded response using the selected language model.\n",
        "\n",
        "This modular chain ensures a clear separation between retrieval, prompting, and generation stages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zc_neG3jmyBS"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retriever,\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXelp47xVGSn"
      },
      "source": [
        "## 10. Graphical User Interface (GUI) for the Study Assistant\n",
        "\n",
        "This step extends the RAG-based study assistant with a simple and user-friendly graphical interface using Gradio.  \n",
        "The interface allows users to interact with the chatbot by entering questions through a text box and receiving responses in real time.\n",
        "\n",
        "This GUI demonstrates the practical usability of the system while relying on the same underlying RAG pipeline without any modification to the retrieval or generation components.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wI9jSHt-VRlm"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def respond(message, history):\n",
        "    if message.strip() == \"\":\n",
        "        return history, \"\"\n",
        "    answer = rag_chain.invoke(message)\n",
        "    history.append((message, answer))\n",
        "    return history, \"\"\n",
        "\n",
        "with gr.Blocks(\n",
        "    title=\"Generative AI Chatbot Assistant\",\n",
        "    theme=gr.themes.Soft(),\n",
        "    css=\"\"\"\n",
        "        body {\n",
        "            background: linear-gradient(135deg, #eef2ff, #f4f7fb);\n",
        "        }\n",
        "\n",
        "        .platform-header {\n",
        "            background: linear-gradient(90deg, #7f7cff, #a0a0ff); /* lighter gradient for logo */\n",
        "            padding: 30px;\n",
        "            border-radius: 14px;\n",
        "            color: white;\n",
        "            text-align: center;\n",
        "            margin-bottom: 25px;\n",
        "            box-shadow: 0 8px 20px rgba(0,0,0,0.12);\n",
        "        }\n",
        "\n",
        "        .platform-header h1 {\n",
        "            margin-bottom: 6px;\n",
        "            font-size: 32px;\n",
        "        }\n",
        "\n",
        "        .platform-header p {\n",
        "            font-size: 15px;\n",
        "            opacity: 0.95;\n",
        "            max-width: 700px;\n",
        "            margin: 0 auto;\n",
        "        }\n",
        "\n",
        "        .chat-card {\n",
        "            background: white;\n",
        "            border-radius: 14px;\n",
        "            padding: 20px;\n",
        "            box-shadow: 0 10px 30px rgba(0,0,0,0.08);\n",
        "        }\n",
        "\n",
        "        .footer-text {\n",
        "            text-align: center;\n",
        "            font-size: 12px;\n",
        "            color: #6b7280;\n",
        "            margin-top: 15px;\n",
        "        }\n",
        "\n",
        "        /* Send Button aligned with textbox */\n",
        "        button.primary {\n",
        "            background: linear-gradient(90deg, #7f7cff, #a0a0ff) !important;\n",
        "            border: none !important;\n",
        "            padding: 6px 16px !important;\n",
        "            font-size: 13px !important;\n",
        "            min-height: 36px !important;\n",
        "            min-width: 100px !important;\n",
        "        }\n",
        "    \"\"\"\n",
        ") as demo:\n",
        "\n",
        "    # Header with Logo\n",
        "    with gr.Column(elem_classes=\"platform-header\"):\n",
        "        gr.Image(\n",
        "            value=\"LogoGUI.png\",\n",
        "            height=100,\n",
        "            show_label=False,\n",
        "            show_download_button=False,\n",
        "            container=False\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            <h1>Generative AI Study Assistant</h1>\n",
        "            <p>\n",
        "                A professional retrieval-augmented learning platform that delivers\n",
        "                accurate, context-aware answers from approved academic resources.\n",
        "            </p>\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    # Chat card\n",
        "    with gr.Column(elem_classes=\"chat-card\"):\n",
        "        chatbot = gr.Chatbot(\n",
        "            label=\"AI Assistant\",\n",
        "            height=300,\n",
        "            show_copy_button=True\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            user_input = gr.Textbox(\n",
        "                placeholder=\"Ask a question about the course content...\",\n",
        "                label=\"Your Question\",\n",
        "                scale=4\n",
        "            )\n",
        "            send_btn = gr.Button(\n",
        "                \"Send\",\n",
        "                variant=\"primary\",\n",
        "                scale=0.1\n",
        "            )\n",
        "\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        <div class=\"footer-text\">\n",
        "            Powered by Retrieval-Augmented Generation (RAG).\n",
        "            Responses are generated exclusively from approved academic resources.\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Event handlers\n",
        "    send_btn.click(\n",
        "        fn=respond,\n",
        "        inputs=[user_input, chatbot],\n",
        "        outputs=[chatbot, user_input]\n",
        "    )\n",
        "\n",
        "    user_input.submit(\n",
        "        fn=respond,\n",
        "        inputs=[user_input, chatbot],\n",
        "        outputs=[chatbot, user_input]\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04Q7Tv1Arajw"
      },
      "source": [
        "## 10. Interactive Chatbot Interface\n",
        "\n",
        "In this final step, an interactive command-line chatbot is implemented.  \n",
        "Users can dynamically input questions related to the course material, and the system responds in real time using the constructed RAG pipeline.\n",
        "\n",
        "This interface demonstrates the practical application of the system as a study assistant rather than a static question-answering script.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vPkQ-gMBnbQe"
      },
      "outputs": [],
      "source": [
        "print(\"Generative AI Study Chatbot\")\n",
        "print(\"Type your question below.\")\n",
        "print(\"Type 'exit' to stop.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_question = input(\"Your question: \")\n",
        "\n",
        "    if user_question.lower() == \"exit\":\n",
        "        print(\"Chatbot session ended.\")\n",
        "        break\n",
        "\n",
        "    answer = rag_chain.invoke(user_question)\n",
        "\n",
        "    print(\"\\nAnswer:\")\n",
        "    print(answer)\n",
        "    print(\"\\n\" + \"-\" * 60 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB-xsHZcJ1XT"
      },
      "source": [
        "## Short Reflection\n",
        "\n",
        "The RAG-based study assistant performed effectively in retrieving relevant course content and generating grounded, context-aware answers, with prompt engineering playing a key role in improving explanation quality and handling indirect or rephrased questions. One limitation encountered was the selection of local open-source language models: *flan-t5-small* produced fast but often imprecise responses, while *mistralai/Mistral-7B-Instruct-v0.2* delivered stronger reasoning at the cost of very slow inference. After experimentation, *flan-t5-base* provided the best balance between accuracy and response time for local execution. The use of Retrieval-Augmented Generation significantly improved answer reliability by grounding responses in the course materials rather than relying on the modelâ€™s parametric knowledge, which reduced hallucinations and increased the relevance and consistency of the generated answers.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}